{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAREAS DE LIMPIEZA DE DATOS\n",
    "1. **Cambiar las abreviaturas -> ejem: \"u\"->you \"ur->\"your\"**\n",
    "2. Buscar  caracteres Nulls y Remplazar con valor \"0\" (El cero debe ser valor String). Igual queda en Duda y se debria preguntar al tutor y o demas compa;eros\n",
    "3. Buscar las palabras con mayor frecuencia y hacer un grafica o Datavisualitation.\n",
    "4. **Quitar stopwords ejem: the, and, that, a, any, an, be, with...**\n",
    "5. eliminar emojis, URL, hashtags, contracciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('prueba.csv', delimiter = ';', names = ['Conversation_Id', 'Authors', 'Class', 'Conversation' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df['Conversation'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazar | por un espacio ya que solo consideraremos las palabras presentes más no las secuencias de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.replace('|', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir números a palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([num2words(word) if word.isnumeric() and int(word)<1000000000 else word for message in x.split('|') for word in message.split(' ')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazar doble espacio con uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([word.lower() for word in x.split(' ') if word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar abreviaciones del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_dict={\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "    \"whats\":\"what is\",\n",
    "    \"whatre\":\"what are\",\n",
    "    \"whos\":\"who is\",\n",
    "    \"whore\":\"who are\",\n",
    "    \"wheres\":\"where is\",\n",
    "    \"wherere\":\"where are\",\n",
    "    \"whens\":\"when is\",\n",
    "    \"whenre\":\"when are\",\n",
    "    \"hows\":\"how is\",\n",
    "    \"howre\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "    \"im\":\"i am\",\n",
    "    #\"were\":\"we are\",\n",
    "    \"youre\":\"you are\",\n",
    "    \"theyre\":\"they are\",\n",
    "    \"hes\":\"he is\",\n",
    "    \"shes\":\"she is\",\n",
    "    \"thats\":\"that is\",\n",
    "    \"theres\":\"there is\",\n",
    "    \"therere\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "    \"ive\":\"i have\",\n",
    "    \"weve\":\"we have\",\n",
    "    \"youve\":\"you have\",\n",
    "    \"theyve\":\"they have\",\n",
    "    \"whove\":\"who have\",\n",
    "    \"wouldve\":\"would have\",\n",
    "    \"notve\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "    \"ill\":\"i will\",\n",
    "    #\"well\":\"we will\",\n",
    "    \"youll\":\"you will\",\n",
    "    \"hell\":\"he will\",\n",
    "    \"shell\":\"she will\",\n",
    "    \"itll\":\"it will\",\n",
    "    \"theyll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    \"isnt\":\"is not\",\n",
    "    \"wasnt\":\"was not\",\n",
    "    \"arent\":\"are not\",\n",
    "    \"werent\":\"were not\",\n",
    "    \"cant\":\"can not\",\n",
    "    \"couldnt\":\"could not\",\n",
    "    \"dont\":\"do not\",\n",
    "    \"didnt\":\"did not\",\n",
    "    \"shouldnt\":\"should not\",\n",
    "    \"wouldnt\":\"would not\",\n",
    "    \"doesnt\":\"does not\",\n",
    "    \"havent\":\"have not\",\n",
    "    \"hasnt\":\"has not\",\n",
    "    \"hadnt\":\"had not\",\n",
    "    \"wont\":\"will not\",\n",
    "\n",
    "    \"2mrw\": \"tomorrow\",\n",
    "    \"aka\": \"also known as\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"a/s/l\": \"age sex location\",\n",
    "    \"ayt\": \"are you there\",\n",
    "    \"b4\": \"before\",\n",
    "    \"bbs\": \"be back soon\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"cmb\": \"call me back\",\n",
    "    \"cu l8r\": \"see you later\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"cos\": \"because\",\n",
    "    \"cwyl\": \"chat with you later\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"f2f\": \"face to face\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"ig\": \"instagram\",\n",
    "    \"fyeo\": \"for your eyes only\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"diy\": \"do it yourself\",\n",
    "    \"stfu\": \"shut the fuck up\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"ily\": \"ily\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"lmfao\": \"laughing my freaking ass off\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"ofc\": \"of course\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"k\": \"okay\",\n",
    "    \"r\": \"are\",\n",
    "    \"n\": \"and\",\n",
    "    \"b\": \"be\",\n",
    "    \"wat\": \"what\",\n",
    "    \"ya\": \"you\",\n",
    "    \"dunno\": \"do not know\",\n",
    "    \"yea\": \"yeah\",\n",
    "    \"ok\": \"okay\",\n",
    "    \"kewl\": \"cool\",\n",
    "    \"nite\": \"night\",\n",
    "    \"yep\": \"yes\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(\"abbreviations_english.json\", \"w\")\n",
    "json.dump(abbr_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../app/api/util/abbreviations_english.json\") as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u': 'you',\n",
       " 'ur': 'your',\n",
       " \"what's\": 'what is',\n",
       " \"what're\": 'what are',\n",
       " \"who's\": 'who is',\n",
       " \"who're\": 'who are',\n",
       " \"where's\": 'where is',\n",
       " \"where're\": 'where are',\n",
       " \"when's\": 'when is',\n",
       " \"when're\": 'when are',\n",
       " \"how's\": 'how is',\n",
       " \"how're\": 'how are',\n",
       " 'whats': 'what is',\n",
       " 'whatre': 'what are',\n",
       " 'whos': 'who is',\n",
       " 'whore': 'who are',\n",
       " 'wheres': 'where is',\n",
       " 'wherere': 'where are',\n",
       " 'whens': 'when is',\n",
       " 'whenre': 'when are',\n",
       " 'hows': 'how is',\n",
       " 'howre': 'how are',\n",
       " \"i'm\": 'i am',\n",
       " \"we're\": 'we are',\n",
       " \"you're\": 'you are',\n",
       " \"they're\": 'they are',\n",
       " \"it's\": 'it is',\n",
       " \"he's\": 'he is',\n",
       " \"she's\": 'she is',\n",
       " \"that's\": 'that is',\n",
       " \"there's\": 'there is',\n",
       " \"there're\": 'there are',\n",
       " 'im': 'i am',\n",
       " 'youre': 'you are',\n",
       " 'theyre': 'they are',\n",
       " 'hes': 'he is',\n",
       " 'shes': 'she is',\n",
       " 'thats': 'that is',\n",
       " 'theres': 'there is',\n",
       " 'therere': 'there are',\n",
       " \"i've\": 'i have',\n",
       " \"we've\": 'we have',\n",
       " \"you've\": 'you have',\n",
       " \"they've\": 'they have',\n",
       " \"who've\": 'who have',\n",
       " \"would've\": 'would have',\n",
       " \"not've\": 'not have',\n",
       " 'ive': 'i have',\n",
       " 'weve': 'we have',\n",
       " 'youve': 'you have',\n",
       " 'theyve': 'they have',\n",
       " 'whove': 'who have',\n",
       " 'wouldve': 'would have',\n",
       " 'notve': 'not have',\n",
       " \"i'll\": 'i will',\n",
       " \"we'll\": 'we will',\n",
       " \"you'll\": 'you will',\n",
       " \"he'll\": 'he will',\n",
       " \"she'll\": 'she will',\n",
       " \"it'll\": 'it will',\n",
       " \"they'll\": 'they will',\n",
       " 'ill': 'i will',\n",
       " 'youll': 'you will',\n",
       " 'hell': 'he will',\n",
       " 'shell': 'she will',\n",
       " 'itll': 'it will',\n",
       " 'theyll': 'they will',\n",
       " \"isn't\": 'is not',\n",
       " \"wasn't\": 'was not',\n",
       " \"aren't\": 'are not',\n",
       " \"weren't\": 'were not',\n",
       " \"can't\": 'can not',\n",
       " \"couldn't\": 'could not',\n",
       " \"don't\": 'do not',\n",
       " \"didn't\": 'did not',\n",
       " \"shouldn't\": 'should not',\n",
       " \"wouldn't\": 'would not',\n",
       " \"doesn't\": 'does not',\n",
       " \"haven't\": 'have not',\n",
       " \"hasn't\": 'has not',\n",
       " \"hadn't\": 'had not',\n",
       " \"won't\": 'will not',\n",
       " 'isnt': 'is not',\n",
       " 'wasnt': 'was not',\n",
       " 'arent': 'are not',\n",
       " 'werent': 'were not',\n",
       " 'cant': 'can not',\n",
       " 'couldnt': 'could not',\n",
       " 'dont': 'do not',\n",
       " 'didnt': 'did not',\n",
       " 'shouldnt': 'should not',\n",
       " 'wouldnt': 'would not',\n",
       " 'doesnt': 'does not',\n",
       " 'havent': 'have not',\n",
       " 'hasnt': 'has not',\n",
       " 'hadnt': 'had not',\n",
       " 'wont': 'will not',\n",
       " '2mrw': 'tomorrow',\n",
       " 'aka': 'also known as',\n",
       " 'asap': 'as soon as possible',\n",
       " 'a/s/l': 'age sex location',\n",
       " 'ayt': 'are you there',\n",
       " 'b4': 'before',\n",
       " 'bbs': 'be back soon',\n",
       " 'bf': 'boyfriend',\n",
       " 'gf': 'girlfriend',\n",
       " 'brb': 'be right back',\n",
       " 'cmb': 'call me back',\n",
       " 'cu l8r': 'see you later',\n",
       " 'cul8r': 'see you later',\n",
       " 'cuz': 'because',\n",
       " 'cos': 'because',\n",
       " 'cwyl': 'chat with you later',\n",
       " 'dm': 'direct message',\n",
       " 'f2f': 'face to face',\n",
       " 'fb': 'facebook',\n",
       " 'ig': 'instagram',\n",
       " 'fyeo': 'for your eyes only',\n",
       " 'fyi': 'for your information',\n",
       " 'diy': 'do it yourself',\n",
       " 'stfu': 'shut the fuck up',\n",
       " 'lmk': 'let me know',\n",
       " 'ily': 'ily',\n",
       " 'yolo': 'you only live once',\n",
       " 'lmfao': 'laughing my freaking ass off',\n",
       " 'nvm': 'never mind',\n",
       " 'ikr': 'i know right',\n",
       " 'ofc': 'of course',\n",
       " 'ttyl': 'talk to you later',\n",
       " 'lol': 'laughing out loud',\n",
       " 'k': 'okay',\n",
       " 'r': 'are',\n",
       " 'n': 'and',\n",
       " 'b': 'be',\n",
       " 'wat': 'what',\n",
       " 'ya': 'you',\n",
       " 'dunno': 'do not know',\n",
       " 'yea': 'yeah',\n",
       " 'ok': 'okay',\n",
       " 'kewl': 'cool',\n",
       " 'nite': 'night',\n",
       " 'yep': 'yes'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../app/api/util/abbreviations_english.json\")\n",
    "d = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u': 'you',\n",
       " 'ur': 'your',\n",
       " \"what's\": 'what is',\n",
       " \"what're\": 'what are',\n",
       " \"who's\": 'who is',\n",
       " \"who're\": 'who are',\n",
       " \"where's\": 'where is',\n",
       " \"where're\": 'where are',\n",
       " \"when's\": 'when is',\n",
       " \"when're\": 'when are',\n",
       " \"how's\": 'how is',\n",
       " \"how're\": 'how are',\n",
       " 'whats': 'what is',\n",
       " 'whatre': 'what are',\n",
       " 'whos': 'who is',\n",
       " 'whore': 'who are',\n",
       " 'wheres': 'where is',\n",
       " 'wherere': 'where are',\n",
       " 'whens': 'when is',\n",
       " 'whenre': 'when are',\n",
       " 'hows': 'how is',\n",
       " 'howre': 'how are',\n",
       " \"i'm\": 'i am',\n",
       " \"we're\": 'we are',\n",
       " \"you're\": 'you are',\n",
       " \"they're\": 'they are',\n",
       " \"it's\": 'it is',\n",
       " \"he's\": 'he is',\n",
       " \"she's\": 'she is',\n",
       " \"that's\": 'that is',\n",
       " \"there's\": 'there is',\n",
       " \"there're\": 'there are',\n",
       " 'im': 'i am',\n",
       " 'youre': 'you are',\n",
       " 'theyre': 'they are',\n",
       " 'hes': 'he is',\n",
       " 'shes': 'she is',\n",
       " 'thats': 'that is',\n",
       " 'theres': 'there is',\n",
       " 'therere': 'there are',\n",
       " \"i've\": 'i have',\n",
       " \"we've\": 'we have',\n",
       " \"you've\": 'you have',\n",
       " \"they've\": 'they have',\n",
       " \"who've\": 'who have',\n",
       " \"would've\": 'would have',\n",
       " \"not've\": 'not have',\n",
       " 'ive': 'i have',\n",
       " 'weve': 'we have',\n",
       " 'youve': 'you have',\n",
       " 'theyve': 'they have',\n",
       " 'whove': 'who have',\n",
       " 'wouldve': 'would have',\n",
       " 'notve': 'not have',\n",
       " \"i'll\": 'i will',\n",
       " \"we'll\": 'we will',\n",
       " \"you'll\": 'you will',\n",
       " \"he'll\": 'he will',\n",
       " \"she'll\": 'she will',\n",
       " \"it'll\": 'it will',\n",
       " \"they'll\": 'they will',\n",
       " 'ill': 'i will',\n",
       " 'youll': 'you will',\n",
       " 'hell': 'he will',\n",
       " 'shell': 'she will',\n",
       " 'itll': 'it will',\n",
       " 'theyll': 'they will',\n",
       " \"isn't\": 'is not',\n",
       " \"wasn't\": 'was not',\n",
       " \"aren't\": 'are not',\n",
       " \"weren't\": 'were not',\n",
       " \"can't\": 'can not',\n",
       " \"couldn't\": 'could not',\n",
       " \"don't\": 'do not',\n",
       " \"didn't\": 'did not',\n",
       " \"shouldn't\": 'should not',\n",
       " \"wouldn't\": 'would not',\n",
       " \"doesn't\": 'does not',\n",
       " \"haven't\": 'have not',\n",
       " \"hasn't\": 'has not',\n",
       " \"hadn't\": 'had not',\n",
       " \"won't\": 'will not',\n",
       " 'isnt': 'is not',\n",
       " 'wasnt': 'was not',\n",
       " 'arent': 'are not',\n",
       " 'werent': 'were not',\n",
       " 'cant': 'can not',\n",
       " 'couldnt': 'could not',\n",
       " 'dont': 'do not',\n",
       " 'didnt': 'did not',\n",
       " 'shouldnt': 'should not',\n",
       " 'wouldnt': 'would not',\n",
       " 'doesnt': 'does not',\n",
       " 'havent': 'have not',\n",
       " 'hasnt': 'has not',\n",
       " 'hadnt': 'had not',\n",
       " 'wont': 'will not',\n",
       " '2mrw': 'tomorrow',\n",
       " 'aka': 'also known as',\n",
       " 'asap': 'as soon as possible',\n",
       " 'a/s/l': 'age sex location',\n",
       " 'ayt': 'are you there',\n",
       " 'b4': 'before',\n",
       " 'bbs': 'be back soon',\n",
       " 'bf': 'boyfriend',\n",
       " 'gf': 'girlfriend',\n",
       " 'brb': 'be right back',\n",
       " 'cmb': 'call me back',\n",
       " 'cu l8r': 'see you later',\n",
       " 'cul8r': 'see you later',\n",
       " 'cuz': 'because',\n",
       " 'cos': 'because',\n",
       " 'cwyl': 'chat with you later',\n",
       " 'dm': 'direct message',\n",
       " 'f2f': 'face to face',\n",
       " 'fb': 'facebook',\n",
       " 'ig': 'instagram',\n",
       " 'fyeo': 'for your eyes only',\n",
       " 'fyi': 'for your information',\n",
       " 'diy': 'do it yourself',\n",
       " 'stfu': 'shut the fuck up',\n",
       " 'lmk': 'let me know',\n",
       " 'ily': 'ily',\n",
       " 'yolo': 'you only live once',\n",
       " 'lmfao': 'laughing my freaking ass off',\n",
       " 'nvm': 'never mind',\n",
       " 'ikr': 'i know right',\n",
       " 'ofc': 'of course',\n",
       " 'ttyl': 'talk to you later',\n",
       " 'lol': 'laughing out loud',\n",
       " 'k': 'okay',\n",
       " 'r': 'are',\n",
       " 'n': 'and',\n",
       " 'b': 'be',\n",
       " 'wat': 'what',\n",
       " 'ya': 'you',\n",
       " 'dunno': 'do not know',\n",
       " 'yea': 'yeah',\n",
       " 'ok': 'okay',\n",
       " 'kewl': 'cool',\n",
       " 'nite': 'night',\n",
       " 'yep': 'yes'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([abbr_dict[word] if word in abbr_dict.keys() else word for word in x.split(' ') ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbers_to_words(text: str) -> str:\n",
    "    cleaned_text = \"\"\n",
    "\n",
    "    for word in text.split(\" \"):\n",
    "        if word.isnumeric() and int(word) < 1000000000:\n",
    "            cleaned_text = f\"{cleaned_text} {num2words(word)}\"\n",
    "        else:\n",
    "            cleaned_text = f\"{cleaned_text} {word}\"\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    #Delete punctuation\n",
    "    text.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "    #Replace double space with only one\n",
    "    text = ' '.join([word.lower() for word in text.split(' ') if word])\n",
    "\n",
    "    #Convert numbers to words\n",
    "    text = ' '.join([num2words(word) if word.isnumeric() and int(word)<1000000000 else word for word in text.split(' ')])\n",
    "\n",
    "    #Replace abbreviations\n",
    "    text = ' '.join([abbr_dict[word] if word in abbr_dict.keys() else word for word in text.split(' ') ])\n",
    "\n",
    "    #Delete punctuation\n",
    "    text.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "    #Delete alphanumeric characters\n",
    "    text = ' '.join([word for word in text.split(' ') if word.isalpha()])\n",
    "\n",
    "    #Lemmatize text\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in text.split(' ')])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar puntuación de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar valores alfanumericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = df_conv.apply(lambda x: ' '.join([word for word in x.split(' ') if word.isalpha()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "df_conv = df_conv.apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in x.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Conversation'] = df_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, naive_bayes, svm\n",
    "x = df['Conversation']\n",
    "y = df['Class']\n",
    "xtrain, xtest, ytrain, ytest = model_selection.train_test_split(x, y, test_size = 0.2, stratify = y, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformación texto a vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000000)\n",
    "vectorizer.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C=1, kernel='linear')#, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def model_evaluate(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.2, stratify = y, random_state = 1)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('SVM', SVM)])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pipeline.pickle', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "def predict(model, text):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    prediction = model.predict(pd.Series(preprocessed_text))\n",
    "\n",
    "    prediction_to_label = {0: 'Negative', 1: 'Positive'}\n",
    "    return prediction_to_label[prediction[0]]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"Do you wanna kiss me?\"\n",
    "\n",
    "    prediction = predict(pipe, text)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM con kernel lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(C=1, kernel='linear')#, class_weight='balanced')\n",
    "Trained = SVM.fit(Xtrain,ytrain)\n",
    "y_pred = SVM.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n",
    "print(classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(Trained, 'modeloOG.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_roc_curve, plot_confusion_matrix\n",
    "plot_confusion_matrix(Trained, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cros-validation (SVM Lineal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate, StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "scoring = ['f1_macro', 'precision_macro', 'recall_macro']\n",
    "cv_results = cross_validate(SVM, Xtrain, ytrain, cv=kfold, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cv_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f F1-score with a standard deviation of %0.2f\" % (cv_results['test_f1_macro'].mean(), cv_results['test_f1_macro'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f precision with a standard deviation of %0.2f\" % (cv_results['test_precision_macro'].mean(), cv_results['test_precision_macro'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f recall with a standard deviation of %0.2f\" % (cv_results['test_recall_macro'].mean(), cv_results['test_recall_macro'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline con DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_clf.fit(Xtrain, ytrain)\n",
    "dummy_pred = dummy_clf.predict(Xtest)\n",
    "print(classification_report(ytest,dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM con kernel RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM3 = svm.SVC(C=10, kernel='rbf', gamma = 'scale')\n",
    "Trained2 = SVM3.fit(Xtrain,ytrain)\n",
    "y_pred3 = SVM3.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(ytest,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Trained2, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results3 = cross_validate(SVM3, Xtrain, ytrain, cv=kfold, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cv_results3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f F1-score with a standard deviation of %0.2f\" % (cv_results3['test_f1_macro'].mean(), cv_results3['test_f1_macro'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f precision with a standard deviation of %0.2f\" % (cv_results3['test_precision_macro'].mean(), cv_results3['test_precision_macro'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.2f recall with a standard deviation of %0.2f\" % (cv_results3['test_recall_macro'].mean(), cv_results3['test_recall_macro'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corriendo el modelo con los datos de entrenamiento para analizar overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = SVM3.predict(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(ytrain,y_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corriendo el modelo con otro data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('test.csv', delimiter = ';', names = ['Conversation_Id', 'Authors', 'Class', 'Conversation' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv2 = df2['Conversation'].dropna()\n",
    "df_conv2 = df_conv2.apply(lambda x: x.replace('|', ' '))\n",
    "df_conv2 = df_conv2.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_conv2 = df_conv2.apply(lambda x: ' '.join([num2words(word) if word.isnumeric() and int(word)<1000000000 else word for message in x.split('|') for word in message.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv2 = df_conv2.apply(lambda x: ' '.join([word.lower() for word in x.split(' ') if word]))\n",
    "df_conv2 = df_conv2.apply(lambda x: ' '.join([abbr_dict[word] if word in abbr_dict.keys() else word for word in x.split(' ') ]))\n",
    "df_conv2 = df_conv2.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_conv2 = df_conv2.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df_conv2 = df_conv2.apply(lambda x: ' '.join([word for word in x.split(' ') if word.isalpha()]))\n",
    "df_conv2 = df_conv2.apply(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in x.split(' ')]))\n",
    "df2['Conversation'] = df_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.dropna(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = df2['Conversation']\n",
    "y2 = df2['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest2 = vectorizer.transform(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xtest2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = SVM3.predict(Xtest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y2,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hat2 = SVM.predict(Xtest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y2,y_hat2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0da68cf08794a047673aec4decc38c851fd2874905e1c48109d4bdcda1a884b3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp-flask')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
